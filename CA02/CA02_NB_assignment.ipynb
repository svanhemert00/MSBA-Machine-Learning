{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCZYXwtCsL_y"
   },
   "source": [
    "# CA02: This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm. \n",
    "\n",
    "In this assignment you will ...\n",
    "1. Complete the code such a way that it works correctly with this given parts of the program.\n",
    "2. Explain as clearly as possible what each part of the code is doing. Use \"Markdown\" texts and code commenting to explain the code\n",
    "\n",
    "__IMPORTANT NOTE:__\n",
    "\n",
    "The path of your data folders 'train-mails' and 'test-mails' must be './train-mails' and './test-mails'. This means you must have your .ipynb file and these folders in the SAME FOLDER in your laptop or Google Drive. The reason for doing this is, this way the peer reviewes and I would be able to run your code from our computers using this exact same relative path, irrespective of our folder hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p_DvtT7sOIr",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Import all other necessary libraries. Your code below ...\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os is not installed\n",
      "numpy==1.22.1\n",
      "collections is not installed\n",
      "pandas==1.4.0\n",
      "sklearn is not installed\n",
      "matplotlib==3.5.1\n",
      "itertools is not installed\n",
      "pkg_resources is not installed\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "def print_versions():\n",
    "    packages = ['os', 'numpy', 'collections', 'pandas', 'sklearn', 'matplotlib', 'itertools', 'pkg_resources']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            package_version = pkg_resources.get_distribution(package).version\n",
    "            print(f'{package}=={package_version}')\n",
    "        except:\n",
    "            print(f'{package} is not installed')\n",
    "\n",
    "print_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function `make_Dictionary` that takes a root directory `(root_dir)` as input. The purpose of this function is to create a dictionary of most common words from a set of text files stored in the specified root directory.\n",
    "\n",
    "The steps performed by this function are as follows:\n",
    "\n",
    "1. Initialize an empty list `all_words` to store all words from the text files.\n",
    "2. Create a list of email file paths (`emails`) by joining the root_dir and filenames obtained by calling `os.listdir(root_dir)`.\n",
    "3. For each email file path in `emails`, open the file and read its content line by line.\n",
    "4. For each line in the email file, split the line into words using the `.split()` method, and add these words to the `all_words` list.\n",
    "5. Create a dictionary `dictionary` using the `Counter` method, which will store the count of each word in the `all_words` list.\n",
    "6. Create a list `list_to_remove` containing all the keys in the dictionary.\n",
    "7. Loop through `list_to_remove`, and remove the keys that are either not alphabetic or have only one character.\n",
    "8. Retain the 3000 most common words in the dictionary using the `most_common` method, and return the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjKF0nIMwz8_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary \n",
    "def make_Dictionary(root_dir):\n",
    "    \n",
    "  all_words = []\n",
    "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
    "    \n",
    "  for mail in emails:\n",
    "    with open(mail) as m:\n",
    "      for line in m:\n",
    "        words = line.split()\n",
    "        all_words += words\n",
    "        \n",
    "  dictionary = Counter(all_words)\n",
    "  list_to_remove = list(dictionary)\n",
    "\n",
    "  for item in list_to_remove:\n",
    "    if item.isalpha() == False:\n",
    "      del dictionary[item]\n",
    "    elif len(item) == 1:\n",
    "      del dictionary[item]\n",
    "    \n",
    "  dictionary = dictionary.most_common(3000)\n",
    "  return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function `extract_features` that takes a directory `(mail_dir)` as input. The purpose of this function is to extract features from text files stored in the specified directory.\n",
    "\n",
    "The steps performed by this function are as follows:\n",
    "\n",
    "1. Create a list `files` of file paths by joining the mail_dir and filenames obtained by calling os.listdir(mail_dir).\n",
    "2. Initialize a feature matrix `features_matrix` with dimensions `(len(files), 3000)` as a numpy array of zeros. This matrix will store the frequency of the most common 3000 words in each text file.\n",
    "3. Initialize an array `train_labels` with length `len(files)` as a numpy array of zeros. This array will store the labels for each text file, where a label of 0 indicates a non-spam email and a label of 1 indicates a spam email.\n",
    "4. Initialize a counter `count` to keep track of the number of spam emails, and initialize a variable `docID` to keep track of the current file index.\n",
    "5. Loop through the `files` list, and for each file:\n",
    "    * Open the file and read its content line by line.\n",
    "    * For the 3rd line of the file, split the line into words using the `.split()` method, and for each word:\n",
    "        * Initialize `wordID` to 0.\n",
    "        * Loop through the dictionary list and check if the word is present in the dictionary. If yes, set the   `wordID` to the index of the word in the dictionary and increment the corresponding entry in the `features_matrix` for the current file by the count of the word in the current line.\n",
    "    * Set the label of the current file to 0 in the `train_labels` array.\n",
    "    * If the last token of the file path starts with \"`spmsg`\", set the label to 1, increment the counter `count`, and update the `train_labels` array accordingly.\n",
    "    * Increment the `docID` by 1.\n",
    "6. Return the `features_matrix` and `train_labels` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmVW5xNlyOFc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(mail_dir):\n",
    "    \n",
    "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "  features_matrix = np.zeros((len(files),3000))\n",
    "  train_labels = np.zeros(len(files))\n",
    "  count = 1;\n",
    "  docID = 0;\n",
    "\n",
    "  for fil in files:\n",
    "    with open(fil) as fi:\n",
    "      for i, line in enumerate(fi):\n",
    "        if i ==2:\n",
    "          words = line.split()\n",
    "          for word in words:\n",
    "            wordID = 0\n",
    "            for i, d in enumerate(dictionary):\n",
    "              if d[0] == word:\n",
    "                wordID = i\n",
    "                features_matrix[docID,wordID] = words.count(word)\n",
    "      train_labels[docID] = 0;\n",
    "      filepathTokens = fil.split('/')\n",
    "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
    "      if lastToken.startswith(\"spmsg\"):\n",
    "        train_labels[docID] = 1;\n",
    "        count = count + 1\n",
    "      docID = docID + 1\n",
    "    \n",
    "  return features_matrix, train_labels                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines two string variables `TRAIN_DIR` and `TEST_DIR` that represent the paths to two directories.\n",
    "\n",
    "`TRAIN_DIR` is the path to the directory that contains training email data, while `TEST_DIR` is the path to the directory that contains test email data.\n",
    "\n",
    "Both directories are part of a larger file system path, which is specified as:\n",
    "`MSBA Machine Learning/Computer Assignments/CA02 - Spam eMail Detection using Naive Bayes Classification Algorithm/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoq-rE7Mx0pp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Enter the \"path\" of your \"train_mails\" and \"test-mails\" FOLDERS in this cell...\n",
    "\n",
    "# for example: \n",
    "TRAIN_DIR = './train-mails'\n",
    "TEST_DIR = './test-mails'\n",
    "\n",
    "#TRAIN_DIR = \"/home/jupyter-svanhemert00/MSBA_ML/computer_assignments/CA02/train-mails\"\n",
    "#TEST_DIR = \"/home/jupyter-svanhemert00/MSBA_ML/computer_assignments/CA02/test-mails\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calls two functions, `make_Dictionary` and `extract_features`, and uses their returned values to create several variables.\n",
    "\n",
    "`make_Dictionary` function is called with `TRAIN_DIR` as the input, which is a directory path to the training emails. This function returns a dictionary object, which is a list of the most common 3000 words found in the emails in the training directory.\n",
    "\n",
    "`extract_features` function is then called twice, once with `TRAIN_DIR` and once with `TEST_DIR`, as the inputs. This function returns two arrays, `features_matrix` and `labels` for the training directory, and `test_features_matrix` and `test_labels` for the test directory. These arrays represent the features and labels of the emails in the directories, respectively.\n",
    "\n",
    "The output \"`reading and processing emails from TRAIN and TEST folders`\" is just a message to the user to indicate that the code is currently processing the emails from both the training and test folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing emails from TRAIN and TEST folders\n"
     ]
    }
   ],
   "source": [
    "dictionary = make_Dictionary(TRAIN_DIR)\n",
    "\n",
    "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
    "features_matrix, labels = extract_features(TRAIN_DIR)\n",
    "test_features_matrix, test_labels = extract_features(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_matrix:\n",
      "[[5. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 5. 0. ... 0. 0. 0.]\n",
      " [3. 0. 2. ... 0. 0. 0.]]\n",
      "\n",
      "test_features_matrix:\n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "labels:\n",
      "[1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.\n",
      " 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1.]\n",
      "\n",
      "test_labels:\n",
      "[1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('features_matrix:')\n",
    "print(features_matrix)\n",
    "print()\n",
    "print('test_features_matrix:')\n",
    "print(test_features_matrix)\n",
    "print()\n",
    "print('labels:')\n",
    "print(labels)\n",
    "print()\n",
    "print('test_labels:')\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "In this section enter your code to TRAIN the model using Naive Bayes algorithm, then PREDICT and then evaluate PERFORMANCE (Accuracy)\n",
    "\n",
    "Your code below ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an ML Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [],
   "source": [
    "# importing the Naive Bayes algorithm module Gaussian which is used in classification\n",
    "# it assumes that features follow normal distribution\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "# using Gaussian Naive Bayes Algorithm\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using Training Dataset \"Features `(X_train)`\" and \"Outcome `(y_train)`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model using Gaussian Naive Bayes algorithm .....\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "# training Naive bayes classifier\n",
    "print (\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
    "model.fit(features_matrix, labels)\n",
    "print (\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model by predicting from the Test Data \"Features (X_test)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing trained model to predict Test Data labels\n",
      "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n"
     ]
    }
   ],
   "source": [
    "# test the unseen passangers from test dataset\n",
    "print (\"testing trained model to predict Test Data labels\")\n",
    "y_predicted = model.predict(test_features_matrix)\n",
    "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Plot the decision boundary\\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\\nZ = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\\nZ = Z.reshape(xx.shape)\\nplt.contourf(xx, yy, Z, alpha=0.5)\\n\\n# Plot the data points\\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('coolwarm', 2))\\nplt.show()\\n\\n# This will generate a scatter plot of the data points colored according to the class labels,\\n# and the decision boundary of the model as a contour plot.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, I wanted to plot the naive bayes model using a matplotlib scatterplot\n",
    "# this code was generated using ChatGPT\n",
    "\n",
    "'''\n",
    "# Plot the decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.5)\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('coolwarm', 2))\n",
    "plt.show()\n",
    "\n",
    "# This will generate a scatter plot of the data points colored according to the class labels,\n",
    "# and the decision boundary of the model as a contour plot.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a 2-class Naive Bayes model trained on 2-dimensional data, you can create a scatter plot of the data points and color-code them based on the class labels. Then you can plot the decision boundary of the model as a contour line separating the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the performance by comparing `y_predicted` against `test_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Labels accuracy_score: 0.9615384615384616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print accuracy of the model\n",
    "\n",
    "# in multilabel classification, this function computes subset accuracy\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# accuracy score is just percentage of correct predictions\n",
    "print(' Test Labels accuracy_score:',accuracy_score(test_labels, y_predicted))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9615384615384616\n",
      "\n",
      "Confusion Matrix:\n",
      " [[129   1]\n",
      " [  9 121]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.99      0.96       130\n",
      "         1.0       0.99      0.93      0.96       130\n",
      "\n",
      "    accuracy                           0.96       260\n",
      "   macro avg       0.96      0.96      0.96       260\n",
      "weighted avg       0.96      0.96      0.96       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation metrics\n",
    "\n",
    "# download evaluation metrics from Scikit-Learn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Calculate accuracy: in multilabel classification, this function computes subset accuracy\n",
    "accuracy = accuracy_score(test_labels, y_predicted)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print()\n",
    "\n",
    "# Generate the confusion matrix\n",
    "confusion_matrix = confusion_matrix(test_labels, y_predicted)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix)\n",
    "print()\n",
    "\n",
    "# Generate the classification report\n",
    "class_report = classification_report(test_labels, y_predicted)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.imshow(confusion_matrix, cmap=plt.cm.Blues)\\nplt.colorbar()\\nplt.xlabel(\"Predicted Class\")\\nplt.ylabel(\"True Class\")\\n\\nclasses = np.unique(test_labels)\\ntick_marks = np.arange(len(classes))\\nplt.xticks(tick_marks, y_predicted, rotation=45)\\nplt.yticks(tick_marks, test_labels)\\n\\nthresh = confusion_matrix.max() / 2.\\nfor i, j in itertools.product(range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])):\\n    plt.text(j, i, confusion_matrix[i, j],\\n             horizontalalignment=\"center\",\\n             color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\\n\\nplt.title(\"Confusion Matrix\")\\nplt.tight_layout()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, I wanted to plot the correlation matrix using a matplotlib heatmap, but I am having trouble doing so.\n",
    "# this code was generated using ChatGPT\n",
    "\n",
    "'''\n",
    "plt.imshow(confusion_matrix, cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "\n",
    "classes = np.unique(test_labels)\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, y_predicted, rotation=45)\n",
    "plt.yticks(tick_marks, test_labels)\n",
    "\n",
    "thresh = confusion_matrix.max() / 2.\n",
    "for i, j in itertools.product(range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])):\n",
    "    plt.text(j, i, confusion_matrix[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In conclusion, we were able to create a notebook that successfully detects spam emails from a normal emails at 96% accuracy. In doing so, we created functions that read train and text files from our directory and pre-processed the data by removing stopwords, non-alphabetical characters, one-letter words, and by tokenizing and vectorizing the remaining words. After creating our train and test features and labels, we used the Scikit-Learn library to run a Naive Bayes classification model on our data and print evaluation metrics. \n",
    "\n",
    "The evaluation metrics indicate that:\n",
    "- __Accuracy__: 96% of our predictions match that of `y_test`\n",
    "- We had 1 false positive and 9 false negatives, meaning that 1 non-spam email was labeled as spam and 9 spam emails were labeled as non-spam. In this use-case, I believe Type I Statistical Errors present a larger risk given that one might be expecting an important, clean email that is no where to be found because it was automatically sent to the spam folder.\n",
    "- __Precision__: 99% indicates that 129 out of 130 of actual positives were detected\n",
    "- __Recall__: 93% indicates out of the ones called positive, 93% of them were correct\n",
    "\n",
    "There are other evaluation metrics that could have been used and I still need help understanding these metrics thouroughly\n",
    "\n",
    "In the future, a good idea would have been to conduct lemmatization or stemming on our features (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5_mPrvN586A"
   },
   "source": [
    "======================= END OF PROGRAM ========================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOaSi3qlFUlqTup/1esXCKD",
   "collapsed_sections": [],
   "name": "naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
